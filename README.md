# Transcriptify 

This application uses a combination of powerful AI technologies to extract and process video transcripts, and generate contextual answers to user questions. This project uses various different services that have each been carefully chosen and tested.

1. **Whisper (OpenAI)**:  
   The application leverages **Whisper**, an automatic speech recognition (ASR) model developed by OpenAI, for **transcribing** the audio from the uploaded video. Whisper is highly effective for transcribing spoken language into text, even in noisy or challenging conditions, ensuring that the content is accurately captured before any analysis is performed. The model is accessed via the `whisper` Python library and processes audio files extracted from the video.

2. **Hugging Face Transformers (Mistral-7B & DistilBERT)**:  
   To handle **text generation** and **sentiment analysis**, the application uses models from the **Hugging Face** ecosystem. For answering questions based on the transcript, the app uses the **Mistral-7B** model, a large language model (LLM) capable of understanding and generating text with a high level of coherence. This model is used for creating detailed responses to questions, considering the relevant parts of the transcript.

   Additionally, **DistilBERT**, a lighter and faster version of BERT, is used for **sentiment analysis** of the generated responses. This helps gauge the tone of the answer, such as whether it is positive, negative, or neutral, by analyzing the first few hundred characters of the generated response.

3. **Groq**:  
   In certain configurations, the application integrates **Groq**, a specialized hardware accelerator for AI models. If an API key for Groq is available, the application can offload text generation tasks to Groq’s cloud-based service, allowing for faster and more efficient processing, especially when working with large language models like **Mixtral**.

4. **YouTube-DL (yt-dlp)**:  
   To download the audio from a video, the application uses **yt-dlp**, a powerful tool for downloading videos and extracting audio from various online platforms, particularly YouTube. This tool ensures the audio is in the correct format for transcription by Whisper.

5. **Prompt Engineering & Testing**:  
   To optimize the quality of answers generated by the LLM, the application employs **prompt engineering**. The prompts are carefully designed to instruct the language model to focus on answering questions using information derived from the video transcript, while minimizing reliance on external knowledge. The system builds a prompt by combining the most relevant transcript chunks and then asks the LLM to respond naturally, ensuring the model provides answers that are detailed and contextually accurate. The prompts are continually tested and refined to get the best results, balancing between coherence and relevance.

6. **LLM Testing**:  
   Throughout the development of the application, **LLM testing** is used to evaluate how well the models handle different types of questions. The goal is to fine-tune the model’s ability to identify and use the most pertinent information from the transcript, while also adjusting for factors like **temperature** and **max tokens** to control the creativity and length of responses. Regular testing ensures that the application provides high-quality answers that are both informative and natural-sounding.

By combining these technologies, the application creates a seamless workflow from video/audio processing to question answering, delivering accurate and contextually appropriate responses that are fine-tuned through rigorous testing of prompts and models.
